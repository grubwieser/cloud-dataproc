{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "test-1.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypYiBwHiXbh5"
      },
      "source": [
        "## 5. Data Ops - Deploy Spark pipeline using Dataproc Workflows\n",
        "\n",
        "### Dataproc Workflows\n",
        "\n",
        "Dataproc Workflows has 2 types of workflow templates.\n",
        "\n",
        "1. Manged cluster - Create a new cluster and delete the cluster once the job has completed.\n",
        "2. Cluster selector - Select a pre-existing Dataproc cluster to the run the jobs (does not delete the cluster).\n",
        "\n",
        "This codelab will use option 1 to create a managed cluster workflow template."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orGUk0kzXbh5"
      },
      "source": [
        "### 5.1 Convert code above into 2 python files\n",
        "\n",
        "1. Job to convert CSV to Hive Tables\n",
        "2. Job to run predictions on Hive Tables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r73UceN6Xbh5",
        "outputId": "c0eb4990-8742-4d08-924b-c056ac87e410"
      },
      "source": [
        "%%writefile job_csv_to_hive.py\n",
        "## Job 1\n",
        "print('Job 1')\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "warehouse_location = 'gs://dataproc-datalake-demo/hive-warehouse'\n",
        "service_endpoint = 'thrift://hive-cluster-m.us-central1-f:9083'\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "  .appName('csv_to_hive') \\\n",
        "  .config(\"hive.metastore.uris\", service_endpoint)  \\\n",
        "  .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
        "  .enableHiveSupport() \\\n",
        "  .getOrCreate()\n",
        "\n",
        "#To-Do add code from notebook job 1 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing job_csv_to_hive.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK2Km8fFXbh5",
        "outputId": "91693cc1-3c13-4ac1-b630-79ba25e8b545"
      },
      "source": [
        "%%writefile job_xgboost_predictions.py\n",
        "## Job 2\n",
        "print('Job 2')\n",
        "\n",
        "# Load the data\n",
        "data = spark.sql(\"\"\"\n",
        "SELECT * \n",
        "FROM bank_demo_db.bank_marketing\n",
        "\"\"\")\n",
        "\n",
        "model_path = 'gs://dataproc-datalake-examples/xgboost/pipeline_model/bank-marketing'\n",
        "\n",
        "loaded_model = XGBoostClassificationModel().load(model_path)\n",
        "\n",
        "loaded_pipeline_model = PipelineModel.load(model_path)\n",
        "\n",
        "#To-Do add code from notebook job 2 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing job_xgboost_predictions.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSHNypqLXbh6"
      },
      "source": [
        "### 5.2. Grant service account permission to deploy workflow from notebooks\n",
        "\n",
        "Dataproc's service accounts needs to be granted \"Dataproc Editor\" IAM role.\n",
        "\n",
        "Go to https://console.cloud.google.com/iam-admin/iam\n",
        "\n",
        "Look for the service account email under the name \"Google Cloud Dataproc Service Agent\". It will be in the format\n",
        "\n",
        "```bash\n",
        "service-{project-number}@dataproc-accounts.iam.gserviceaccount.com\n",
        "```\n",
        "\n",
        "Edit the roles and add the role \"Dataproc Editor\" and press save."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CugSWlAIXbh6"
      },
      "source": [
        "## Alternatively run all of the below from the Google Cloud Shell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttBVmjAMXbh6"
      },
      "source": [
        "### 5.3 Create Dataproc managed cluster workflow Template"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grA7CAj6Xbh6"
      },
      "source": [
        "%%bash\n",
        "export WORKFLOW_ID=bank-marketing-workflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2Hbhjn8Xbh6",
        "outputId": "71e24740-c7d8-46fe-ade0-d1840743dc52"
      },
      "source": [
        "%%bash\n",
        "export WORKFLOW_ID=bank-marketing-workflow\n",
        "echo $WORKFLOW_ID"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bank-marketing-workflow\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-R7YyYZeXbh7"
      },
      "source": [
        "%%bash\n",
        "export WORKFLOW_ID=bank-marketing-workflow\n",
        "export REGION=us-central1\n",
        "\n",
        "gcloud dataproc workflow-templates create $WORKFLOW_ID \\\n",
        "--region $REGION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqyNHtDnXbh7"
      },
      "source": [
        "### 5.4 Configure managed cluster for the workflow template"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l69UqdBMXbh7"
      },
      "source": [
        "%%bash\n",
        "export WORKFLOW_ID=bank-marketing-workflow\n",
        "\n",
        "export PROJECT_ID=dataproc-datalake\n",
        "export CLUSTER_NAME=spark-workflow-cluster\n",
        "export BUCKET_NAME=${PROJECT_ID}-demo\n",
        "export REGION=us-central1\n",
        "export ZONE=us-central1-f\n",
        "\n",
        "gcloud beta dataproc workflow-templates set-managed-cluster $WORKFLOW_ID \\\n",
        "    --cluster-name $CLUSTER_NAME \\\n",
        "    --region $REGION \\\n",
        "    --zone $ZONE \\\n",
        "    --image-version preview-ubuntu \\\n",
        "    --master-machine-type n1-standard-4 \\\n",
        "    --worker-machine-type n1-standard-4 \\\n",
        "    --optional-components=ANACONDA,JUPYTER \\\n",
        "    --initialization-actions gs://goog-dataproc-initialization-actions-${REGION}/rapids/rapids.sh \\\n",
        "    --metadata rapids-runtime=SPARK \\\n",
        "    --bucket $BUCKET_NAME"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_nfCDwyXbh7"
      },
      "source": [
        "### 5.5 Upload PySpark job to GCS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4nLwN6hXbh7",
        "outputId": "28a6384e-93fa-44a5-d80d-3ff981e2f17f"
      },
      "source": [
        "%%bash\n",
        "export PROJECT_ID=dataproc-datalake\n",
        "export BUCKET_NAME=${PROJECT_ID}-demo\n",
        "gsutil cp job_csv_to_hive.py \\\n",
        " gs://${PROJECT_ID}-demo/workflows/spark-bank-marketing/job_csv_to_hive.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://job_csv_to_hive.py [Content-Type=text/x-python]...\n",
            "/ [1 files][   24.0 B/   24.0 B]                                                \n",
            "Operation completed over 1 objects/24.0 B.                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jSsU7qKXbh7",
        "outputId": "4484ed99-ae9d-4473-d4e1-92e1e15d86c1"
      },
      "source": [
        "%%bash\n",
        "export PROJECT_ID=dataproc-datalake\n",
        "export BUCKET_NAME=${PROJECT_ID}-demo\n",
        "gsutil cp job_xgboost_predictions.py \\\n",
        " gs://${PROJECT_ID}-demo/workflows/spark-bank-marketing/job_xgboost_predictions.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://job_xgboost_predictions.py [Content-Type=text/x-python]...\n",
            "/ [1 files][   24.0 B/   24.0 B]                                                \n",
            "Operation completed over 1 objects/24.0 B.                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0OTR3QVXbh7"
      },
      "source": [
        "### 5.6 Add job to workflow template"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atXOS-9hXbh8",
        "outputId": "3cd9dc51-08c9-4b7b-a24f-93d6113493ac"
      },
      "source": [
        "%%bash\n",
        "export WORKFLOW_ID=bank-marketing-workflow\n",
        "export REGION=us-central1\n",
        "export PROJECT_ID=dataproc-datalake\n",
        "\n",
        "gcloud dataproc workflow-templates add-job pyspark \\\n",
        "  gs://${PROJECT_ID}-demo/workflows/spark-bank-marketing/job_csv_to_hive.py \\\n",
        "    --region $REGION \\\n",
        "    --step-id csv_to_hive \\\n",
        "    --workflow-template $WORKFLOW_ID"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "createTime: '2020-09-08T16:56:47.304Z'\n",
            "id: bank-marketing-workflow\n",
            "jobs:\n",
            "- pysparkJob:\n",
            "    mainPythonFileUri: gs://dataproc-datalake-demo/workflows/spark-bank-marketing/job_csv_to_hive.py\n",
            "  stepId: csv_to_hive\n",
            "name: projects/dataproc-datalake/regions/us-central1/workflowTemplates/bank-marketing-workflow\n",
            "placement:\n",
            "  managedCluster:\n",
            "    clusterName: spark-workflow-cluster\n",
            "    config:\n",
            "      configBucket: dataproc-datalake-demo\n",
            "      gceClusterConfig:\n",
            "        metadata:\n",
            "          rapids-runtime: SPARK\n",
            "        zoneUri: us-central1-f\n",
            "      initializationActions:\n",
            "      - executableFile: gs://goog-dataproc-initialization-actions-us-central1/rapids/rapids.sh\n",
            "        executionTimeout: 600s\n",
            "      masterConfig:\n",
            "        machineTypeUri: n1-standard-4\n",
            "      softwareConfig:\n",
            "        imageVersion: preview-ubuntu\n",
            "        optionalComponents:\n",
            "        - ANACONDA\n",
            "        - JUPYTER\n",
            "      workerConfig:\n",
            "        machineTypeUri: n1-standard-4\n",
            "updateTime: '2020-09-08T17:09:28.406Z'\n",
            "version: 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6Pcp33KXbh8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgcSgX7PXbh8",
        "outputId": "efa1a1fc-45eb-4bb3-c2c7-fda2bacdd94f"
      },
      "source": [
        "%%bash\n",
        "export WORKFLOW_ID=bank-marketing-workflow\n",
        "export REGION=us-central1\n",
        "export PROJECT_ID=dataproc-datalake\n",
        "\n",
        "gcloud dataproc workflow-templates add-job pyspark \\\n",
        "  gs://${PROJECT_ID}-demo/workflows/spark-bank-marketing/job_xgboost_predictions.py \\\n",
        "    --region $REGION \\\n",
        "    --start-after=csv_to_hive \\\n",
        "    --step-id xgboost_predictions \\\n",
        "    --workflow-template $WORKFLOW_ID"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "createTime: '2020-09-08T16:56:47.304Z'\n",
            "id: bank-marketing-workflow\n",
            "jobs:\n",
            "- pysparkJob:\n",
            "    mainPythonFileUri: gs://dataproc-datalake-demo/workflows/spark-bank-marketing/job_csv_to_hive.py\n",
            "  stepId: csv_to_hive\n",
            "- prerequisiteStepIds:\n",
            "  - csv_to_hive\n",
            "  pysparkJob:\n",
            "    mainPythonFileUri: gs://dataproc-datalake-demo/workflows/spark-bank-marketing/job_xgboost_predictions.py\n",
            "  stepId: xgboost_predictions\n",
            "name: projects/dataproc-datalake/regions/us-central1/workflowTemplates/bank-marketing-workflow\n",
            "placement:\n",
            "  managedCluster:\n",
            "    clusterName: spark-workflow-cluster\n",
            "    config:\n",
            "      configBucket: dataproc-datalake-demo\n",
            "      gceClusterConfig:\n",
            "        metadata:\n",
            "          rapids-runtime: SPARK\n",
            "        zoneUri: us-central1-f\n",
            "      initializationActions:\n",
            "      - executableFile: gs://goog-dataproc-initialization-actions-us-central1/rapids/rapids.sh\n",
            "        executionTimeout: 600s\n",
            "      masterConfig:\n",
            "        machineTypeUri: n1-standard-4\n",
            "      softwareConfig:\n",
            "        imageVersion: preview-ubuntu\n",
            "        optionalComponents:\n",
            "        - ANACONDA\n",
            "        - JUPYTER\n",
            "      workerConfig:\n",
            "        machineTypeUri: n1-standard-4\n",
            "updateTime: '2020-09-08T17:10:02.855Z'\n",
            "version: 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX4bxu6PXbh8"
      },
      "source": [
        "### 5.7 Run workflow template"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtd1tCXLXbh8",
        "outputId": "9fc709fc-2443-486f-c4dc-403eb4ae334e"
      },
      "source": [
        "%%bash\n",
        "export WORKFLOW_ID=bank-marketing-workflow\n",
        "export REGION=us-central1\n",
        "\n",
        "gcloud dataproc workflow-templates instantiate $WORKFLOW_ID \\\n",
        "--region $REGION"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Waiting on operation [projects/dataproc-datalake/regions/us-central1/operations/4072cbbe-c966-3708-b54d-4298354ab78a].\n",
            "WorkflowTemplate [bank-marketing-workflow] PENDING\n",
            "WorkflowTemplate [bank-marketing-workflow] RUNNING\n",
            "Creating cluster: Operation ID [projects/dataproc-datalake/regions/us-central1/operations/db00b13c-d677-48ae-9f78-f2e29c693d08].\n",
            "Created cluster: spark-workflow-cluster-zobjl64tmvq4s.\n",
            "Job ID csv_to_hive-zobjl64tmvq4s RUNNING\n",
            "Job ID csv_to_hive-zobjl64tmvq4s COMPLETED\n",
            "Job ID xgboost_predictions-zobjl64tmvq4s COMPLETED\n",
            "Deleting cluster: Operation ID [projects/dataproc-datalake/regions/us-central1/operations/0478b08a-944b-4d8a-99ba-1a013662ea2a].\n",
            "WorkflowTemplate [bank-marketing-workflow] DONE\n",
            "Deleted cluster: spark-workflow-cluster-zobjl64tmvq4s.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_7YDgi4Xbh9"
      },
      "source": [
        "### 5.8 View Cluster, workflow and jobs tabs\n",
        "\n",
        "Go to the Dataproc UI and view the cluster page. You should see the new cluster spinning up\n",
        "\n",
        "Once the cluster is ready view the workflow and jobs tabs to check the progress of the jobs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSOrM9OVXbh9"
      },
      "source": [
        "### 5.9 Check new predictions table was created"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYTNNKOOXbh9"
      },
      "source": [
        "### 5.10 Schedule workflows\n",
        "\n",
        "View the guide on how to schedule Dataproc workflows\n",
        "\n",
        "https://cloud.google.com/dataproc/docs/concepts/workflows/workflow-schedule-solutions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7E-0kpEQXbh9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}