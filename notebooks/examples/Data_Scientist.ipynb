{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "test-1.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3TwAK_8Wll0"
      },
      "source": [
        "## 4. Data Scientist - Create ML models with Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwCbd6SqWll0"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJwl3BLZWll0"
      },
      "source": [
        "Create a Spark DataFrame from hive table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClEdzuRoWll0"
      },
      "source": [
        "data = spark.sql(\"\"\"\n",
        "SELECT * \n",
        "FROM bank_demo_db.bank_marketing\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IrwU3frWll0"
      },
      "source": [
        "Cache the DataFrame in memory "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbCBs_1CWll0",
        "outputId": "d4175cd5-fbaa-42dc-c3ca-730737256026"
      },
      "source": [
        "data.cache()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Age: int, Job: string, MaritalStatus: string, Education: string, Default: string, Balance: int, Housing: string, Loan: string, Contact: string, Day: int, Month: string, Duration: int, Campaign: int, PDays: int, Previous: int, POutcome: string, Deposit: int]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDeK2eGgWll1",
        "outputId": "15611190-1664-48da-997c-55cf5e04bead"
      },
      "source": [
        "data.groupBy(\"Deposit\").count().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-----+\n",
            "|Deposit|count|\n",
            "+-------+-----+\n",
            "|      1|39922|\n",
            "|      2| 5289|\n",
            "+-------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnOrzGwzWll1"
      },
      "source": [
        "### Split training and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQzKkubKWll1"
      },
      "source": [
        "(train_data, test_data) = data.randomSplit([0.7, 0.3], seed=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWSFnJLqWll1",
        "outputId": "c6f9f2d9-7078-4d0a-ddcd-6d545b0ceee9"
      },
      "source": [
        "train_data.groupBy(\"Deposit\").count().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-----+\n",
            "|Deposit|count|\n",
            "+-------+-----+\n",
            "|      1|28004|\n",
            "|      2| 3709|\n",
            "+-------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6ov0IxjWll1",
        "outputId": "2457d42b-2ca6-4c74-886e-9a9a7dfb42ab"
      },
      "source": [
        "train_data.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31713"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTfukzr6Wll1",
        "outputId": "6c66c2b0-4634-4ea2-d487-9b9f7a47dcb1"
      },
      "source": [
        "test_data.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13498"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msLwIwYlWll1"
      },
      "source": [
        "## Create Spark ML Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNdmjucvWll2"
      },
      "source": [
        "Train a RandomForestClassifier model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMu3XllwWll2",
        "outputId": "df7bab04-49aa-4032-ad89-5c6173786d00"
      },
      "source": [
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "\n",
        "categorical_cols = [field for (field, data_type) in train_data.dtypes \n",
        "                    if ((data_type == \"string\") & (field != 'Deposit'))]\n",
        "\n",
        "index_output_cols = [x + \"_Index\" for x in categorical_cols]\n",
        "\n",
        "ohe_output_cols = [x + \"_OHE\" for x in categorical_cols]\n",
        "\n",
        "categorical_string_indexer = StringIndexer(\n",
        "    inputCols=categorical_cols,\n",
        "    outputCols=index_output_cols,\n",
        "    handleInvalid=\"skip\")\n",
        "\n",
        "ohe_encoder = OneHotEncoder(\n",
        "    inputCols=index_output_cols,\n",
        "    outputCols=ohe_output_cols)\n",
        "\n",
        "numeric_cols = [field for (field, data_type) in train_data.dtypes \n",
        "                if (((data_type == \"double\") | (data_type == \"int\") | (data_type == \"bigint\"))\n",
        "                  & (field != 'Deposit'))]\n",
        "\n",
        "assembler_inputs = ohe_output_cols + numeric_cols\n",
        "\n",
        "vec_assembler = VectorAssembler(\n",
        "    inputCols=assembler_inputs,\n",
        "    outputCol=\"features\")\n",
        "\n",
        "label_string_indexer = StringIndexer(). \\\n",
        "  setInputCol(\"Deposit\"). \\\n",
        "  setOutputCol(\"label\")\n",
        "\n",
        "# Train a RandomForestClassifier model.\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
        "\n",
        "pipeline = Pipeline(stages=[\n",
        "    categorical_string_indexer,\n",
        "    ohe_encoder,\n",
        "    vec_assembler,\n",
        "    label_string_indexer,\n",
        "    rf\n",
        "])\n",
        "\n",
        "# Train model on training data\n",
        "pipeline_model = pipeline.fit(train_data)\n",
        "\n",
        "# Make predictions on test.\n",
        "predictions = pipeline_model.transform(test_data)\n",
        "\n",
        "# Select example rows to display.\n",
        "predictions.select(\"prediction\", \"label\", \"features\").show(5) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-----+--------------------+\n",
            "|prediction|label|            features|\n",
            "+----------+-----+--------------------+\n",
            "|       0.0|  0.0|(42,[10,12,15,16,...|\n",
            "|       0.0|  0.0|(42,[10,12,16,18,...|\n",
            "|       0.0|  1.0|(42,[10,12,16,18,...|\n",
            "|       0.0|  1.0|(42,[10,12,16,18,...|\n",
            "|       0.0|  0.0|(42,[10,12,15,16,...|\n",
            "+----------+-----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DZwokDnWll2"
      },
      "source": [
        "As the dataset is imbalanced a good metric is AUC: Area Under the ROC Curve. [Learn more about AUC here.](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc#AUC)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvoICebVWll2",
        "outputId": "a54b854e-a7c0-48c6-945b-d340d195f260"
      },
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "binaryEvaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
        "\n",
        "auc = binaryEvaluator.evaluate(predictions, {binaryEvaluator.metricName: \"areaUnderROC\"})\n",
        "print(auc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8867264917867028\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBSZdvX_Wll2",
        "outputId": "3d5c7965-0771-4842-9e9d-26ce7b1f1f71"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
        "\n",
        "predictions_np = np.array((predictions.select(\"label\",\"prediction\").collect()))\n",
        "\n",
        "np_acc = accuracy_score(predictions_np[:,0], predictions_np[:,1])\n",
        "np_f1 = f1_score(predictions_np[:,0], predictions_np[:,1])\n",
        "np_precision = precision_score(predictions_np[:,0], predictions_np[:,1])\n",
        "np_recall = recall_score(predictions_np[:,0], predictions_np[:,1])\n",
        "np_auc = roc_auc_score(predictions_np[:,0], predictions_np[:,1])\n",
        "\n",
        "print(\"f1:\", np_f1)\n",
        "print(\"precision:\", np_precision)\n",
        "print(\"recall:\", np_recall)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f1: 0.08268424206111442\n",
            "precision: 0.7752808988764045\n",
            "recall: 0.043670886075949364\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y46SX-NUWll3",
        "outputId": "213009e8-130e-45ef-dd79-2ab25997e016"
      },
      "source": [
        "# import package that will generate the confusion matrix scores\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# import packages that will help display the scores\n",
        "import pandas as pd\n",
        "\n",
        "confusion_matrix_scores = confusion_matrix(predictions_np[:,0], \n",
        "                                           predictions_np[:,1], \n",
        "                                           labels=[1, 0])\n",
        "\n",
        "# display scores as a heatmap\n",
        "df = pd.DataFrame(confusion_matrix_scores, \n",
        "                  columns = [\"Predicted True\", \"Predicted Not True\"],\n",
        "                  index = [\"Actually True\", \"Actually Not True\"])\n",
        "\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted True</th>\n",
              "      <th>Predicted Not True</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Actually True</th>\n",
              "      <td>69</td>\n",
              "      <td>1511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Actually Not True</th>\n",
              "      <td>20</td>\n",
              "      <td>11898</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   Predicted True  Predicted Not True\n",
              "Actually True                  69                1511\n",
              "Actually Not True              20               11898"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHjqQFEzWll3"
      },
      "source": [
        "## Improve model using XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t46HwGRzWll4"
      },
      "source": [
        "Train model using XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR12PwSoWll4"
      },
      "source": [
        "# spark.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TBbgWKtWll4"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "warehouse_location = 'gs://dataproc-datalake-demo/hive-warehouse'\n",
        "service_endpoint = 'thrift://hive-cluster-m.us-central1-f:9083'\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "  .appName('Hive and XGBoost - GPU') \\\n",
        "  .config('spark.jars.packages', 'com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.17.1') \\\n",
        "  .config(\"spark.rapids.memory.gpu.pooling.enabled\", \"false\") \\\n",
        "  .config(\"spark.executor.instances\", \"4\") \\\n",
        "  .config(\"spark.executor.cores\", \"2\") \\\n",
        "  .config(\"spark.task.cpus\", \"2\") \\\n",
        "  .config(\"spark.task.resource.gpu.amount\", \"1\") \\\n",
        "  .config(\"hive.metastore.uris\", service_endpoint)  \\\n",
        "  .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
        "  .enableHiveSupport() \\\n",
        "  .getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqlyJ0RkWll4",
        "outputId": "0ecb79c6-7457-42f8-9c0c-1eb376453354"
      },
      "source": [
        "spark.conf.get(\"spark.app.id\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'application_1599581036896_0008'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpaVpADqWll4",
        "outputId": "197b5b55-58f2-4089-86c8-7d79a8b09501"
      },
      "source": [
        "data = spark.sql(\"\"\"\n",
        "SELECT * \n",
        "FROM bank_demo_db.bank_marketing\n",
        "\"\"\")\n",
        "\n",
        "(train_data, test_data) = data.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "train_data.cache()\n",
        "train_data.show(3)\n",
        "\n",
        "test_data.cache()\n",
        "test_data.show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-------+-------------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+-------+\n",
            "|Age|    Job|MaritalStatus|Education|Default|Balance|Housing|Loan| Contact|Day|Month|Duration|Campaign|PDays|Previous|POutcome|Deposit|\n",
            "+---+-------+-------------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+-------+\n",
            "| 18|student|       single|  primary|     no|    608|     no|  no|cellular| 12|  aug|     267|       1|   -1|       0| unknown|      2|\n",
            "| 18|student|       single|  primary|     no|    608|     no|  no|cellular| 13|  nov|     210|       1|   93|       1| success|      2|\n",
            "| 18|student|       single|secondary|     no|      5|     no|  no|cellular| 24|  aug|     143|       2|   -1|       0| unknown|      1|\n",
            "+---+-------+-------------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+-------+\n",
            "only showing top 3 rows\n",
            "\n",
            "+---+-------+-------------+---------+-------+-------+-------+----+---------+---+-----+--------+--------+-----+--------+--------+-------+\n",
            "|Age|    Job|MaritalStatus|Education|Default|Balance|Housing|Loan|  Contact|Day|Month|Duration|Campaign|PDays|Previous|POutcome|Deposit|\n",
            "+---+-------+-------------+---------+-------+-------+-------+----+---------+---+-----+--------+--------+-----+--------+--------+-------+\n",
            "| 18|student|       single|  primary|     no|   1944|     no|  no|telephone| 10|  aug|     122|       3|   -1|       0| unknown|      1|\n",
            "| 18|student|       single|  unknown|     no|     35|     no|  no|telephone| 21|  aug|     104|       2|   -1|       0| unknown|      1|\n",
            "| 18|student|       single|  unknown|     no|    108|     no|  no| cellular|  9|  feb|      92|       1|  183|       1| success|      2|\n",
            "+---+-------+-------------+---------+-------+-------+-------+----+---------+---+-----+--------+--------+-----+--------+--------+-------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6sbKErGWll4"
      },
      "source": [
        "## Create ML Pipeline with XGBoost model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzntXehKWll4"
      },
      "source": [
        "from pyspark.ml import Pipeline, PipelineModel\n",
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
        "from ml.dmlc.xgboost4j.scala.spark import XGBoostClassificationModel, XGBoostClassifier\n",
        "\n",
        "categorical_cols = [field for (field, data_type) in train_data.dtypes \n",
        "                    if ((data_type == \"string\") & (field != 'Deposit'))]\n",
        "\n",
        "string_index_output_cols = [x + \"_Index\" for x in categorical_cols]\n",
        "\n",
        "categorical_string_indexer = StringIndexer(\n",
        "    inputCols=categorical_cols,\n",
        "    outputCols=string_index_output_cols,\n",
        "    handleInvalid=\"skip\")\n",
        "\n",
        "numeric_cols = [field for (field, data_type) in train_data.dtypes \n",
        "                if (((data_type == \"double\") | (data_type == \"int\") | (data_type == \"bigint\"))\n",
        "                  & (field != 'Deposit'))]\n",
        "\n",
        "features = string_index_output_cols + numeric_cols\n",
        "\n",
        "label_string_indexer = StringIndexer(). \\\n",
        "  setInputCol(\"Deposit\"). \\\n",
        "  setOutputCol(\"label\")\n",
        "\n",
        "params = { \n",
        "    'treeMethod': 'gpu_hist',\n",
        "    'maxDepth': 10, \n",
        "    'maxLeaves': 256,\n",
        "    'growPolicy': 'depthwise',\n",
        "    'objective': 'binary:logistic',\n",
        "    'numRound': 100,\n",
        "    'numWorkers': 2\n",
        "}\n",
        "\n",
        "# For GPU you must use .setFeaturesCols(features) and pass in the list of columns that are the features\n",
        "xgbc = XGBoostClassifier(**params).setLabelCol(\"label\").setFeaturesCols(features)\n",
        "\n",
        "# For CPU training you must use .setFeaturesCol('features') which \n",
        "# expects the features to be vectorised into one column first\n",
        "# xgbc = XGBoostClassifier(**params).setLabelCol('label').setFeaturesCol('features')\n",
        "\n",
        "pipeline = Pipeline(stages=[\n",
        "    categorical_string_indexer,\n",
        "    label_string_indexer,\n",
        "    xgbc\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxDVWlaQWll4",
        "outputId": "07b79192-fd72-407c-b488-9da50076f8d3"
      },
      "source": [
        "%%time\n",
        "# Train model on training data\n",
        "pipeline_model = pipeline.fit(train_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 74.4 ms, sys: 18.3 ms, total: 92.7 ms\n",
            "Wall time: 20.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDWpZGMgWll5",
        "outputId": "dbd4f464-970a-4201-808f-782375e01c1b"
      },
      "source": [
        "# Make predictions on test\n",
        "\n",
        "predictions = pipeline_model.transform(test_data)\n",
        "predictions.select(\"prediction\", \"label\").show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-----+\n",
            "|prediction|label|\n",
            "+----------+-----+\n",
            "|       0.0|  0.0|\n",
            "|       0.0|  0.0|\n",
            "|       1.0|  1.0|\n",
            "|       0.0|  1.0|\n",
            "|       0.0|  0.0|\n",
            "+----------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N07NxcFuWll5",
        "outputId": "4227e777-3c2c-4be0-d806-de0583cba0d4"
      },
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "binaryEvaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
        "\n",
        "auc = binaryEvaluator.evaluate(predictions, {binaryEvaluator.metricName: \"areaUnderROC\"})\n",
        "print(auc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9261211368401443\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vunD071mWll5"
      },
      "source": [
        "### View model stats using Numpy and Scikit-learn\n",
        "\n",
        "PySpark cannot be used to calculate the precision, recall, and f1_score for binary classification evaluation and therefore sklearn.metrics is used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p281PuvtWll5",
        "outputId": "d48e7421-d890-4107-f7c1-9a24f536217f"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
        "\n",
        "predictions_np = np.array((predictions.select(\"label\",\"prediction\").collect()))\n",
        "\n",
        "np_acc = accuracy_score(predictions_np[:,0], predictions_np[:,1])\n",
        "np_f1 = f1_score(predictions_np[:,0], predictions_np[:,1])\n",
        "np_precision = precision_score(predictions_np[:,0], predictions_np[:,1])\n",
        "np_recall = recall_score(predictions_np[:,0], predictions_np[:,1])\n",
        "np_auc = roc_auc_score(predictions_np[:,0], predictions_np[:,1])\n",
        "\n",
        "print(\"f1:\", np_f1)\n",
        "print(\"precision:\", np_precision)\n",
        "print(\"recall:\", np_recall)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f1: 0.5221674876847291\n",
            "precision: 0.5879556259904913\n",
            "recall: 0.46962025316455697\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwM-2efzWll6",
        "outputId": "77b19fd1-70a4-419b-cd54-c8e09d1bccd1"
      },
      "source": [
        "# import package that will generate the confusion matrix scores\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# import packages that will help display the scores\n",
        "import pandas as pd\n",
        "\n",
        "confusion_matrix_scores = confusion_matrix(predictions_np[:,0], \n",
        "                                           predictions_np[:,1], \n",
        "                                           labels=[1, 0])\n",
        "\n",
        "# display scores as a heatmap\n",
        "df = pd.DataFrame(confusion_matrix_scores, \n",
        "                  columns = [\"Predicted True\", \"Predicted Not True\"],\n",
        "                  index = [\"Actually True\", \"Actually Not True\"])\n",
        "\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted True</th>\n",
              "      <th>Predicted Not True</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Actually True</th>\n",
              "      <td>742</td>\n",
              "      <td>838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Actually Not True</th>\n",
              "      <td>520</td>\n",
              "      <td>11398</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   Predicted True  Predicted Not True\n",
              "Actually True                 742                 838\n",
              "Actually Not True             520               11398"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJylDTrvWll6"
      },
      "source": [
        "### Save model_pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yyu-e73EWll6"
      },
      "source": [
        "from pyspark.ml import Pipeline, PipelineModel\n",
        "\n",
        "model_path = 'gs://dataproc-datalake-examples/xgboost/pipeline_model/bank-marketing'\n",
        "\n",
        "pipeline_model.write().overwrite().save(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOsd-Q_CWll6"
      },
      "source": [
        "loaded_pipeline_model = PipelineModel.load(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RycT75znWll6",
        "outputId": "dbeb5ae8-9713-40a3-86aa-11ed3c6ab8b1"
      },
      "source": [
        "# Make predictions using loaded model\n",
        "\n",
        "predictions = loaded_pipeline_model.transform(test_data)\n",
        "\n",
        "predictions.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-------+---+--------+--------+-----+--------+-------+-----------+---------+-------------------+--------------+-------------+-------------+---------------+----------+-------------+-----+--------------------+--------------------+----------+\n",
            "|Age|Balance|Day|Duration|Campaign|PDays|Previous|Deposit|Month_Index|Job_Index|MaritalStatus_Index|POutcome_Index|Housing_Index|Contact_Index|Education_Index|Loan_Index|Default_Index|label|       rawPrediction|         probability|prediction|\n",
            "+---+-------+---+--------+--------+-----+--------+-------+-----------+---------+-------------------+--------------+-------------+-------------+---------------+----------+-------------+-----+--------------------+--------------------+----------+\n",
            "| 18|   1944| 10|     122|       3|   -1|       0|      1|        2.0|     10.0|                1.0|           0.0|          1.0|          2.0|            2.0|       0.0|          0.0|  0.0|[6.82134294509887...|[0.99891093163751...|       0.0|\n",
            "| 18|     35| 21|     104|       2|   -1|       0|      1|        2.0|     10.0|                1.0|           0.0|          1.0|          2.0|            3.0|       0.0|          0.0|  0.0|[1.91996443271636...|[0.87213446199893...|       0.0|\n",
            "| 18|    108|  9|      92|       1|  183|       1|      2|        6.0|     10.0|                1.0|           3.0|          1.0|          0.0|            3.0|       0.0|          0.0|  1.0|[-2.4723315238952...|[0.07782077789306...|       1.0|\n",
            "| 18|    108| 10|     167|       1|   -1|       0|      2|        2.0|     10.0|                1.0|           0.0|          1.0|          0.0|            3.0|       0.0|          0.0|  1.0|[0.12223700433969...|[0.53052130341529...|       0.0|\n",
            "| 19|     56| 12|     246|       1|   -1|       0|      1|        2.0|     10.0|                1.0|           0.0|          1.0|          0.0|            2.0|       0.0|          0.0|  0.0|[2.10480499267578...|[0.89136932045221...|       0.0|\n",
            "+---+-------+---+--------+--------+-----+--------+-------+-----------+---------+-------------------+--------------+-------------+-------------+---------------+----------+-------------+-----+--------------------+--------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtPkjnCjWll6"
      },
      "source": [
        "### Save prediction results to a new table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "052I5XQVWll6"
      },
      "source": [
        "predictions.write.mode('overwrite').format(\"parquet\").saveAsTable(\"bank_demo_db.bank_marketing_predictions\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhANol4oWll7",
        "outputId": "0635343e-d6c5-4dd7-e654-f3c77b7eaa01"
      },
      "source": [
        "spark.sql(\"SHOW TABLES in bank_demo_db\").show(10, False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+--------------------------+-----------+\n",
            "|database    |tableName                 |isTemporary|\n",
            "+------------+--------------------------+-----------+\n",
            "|bank_demo_db|bank_marketing            |false      |\n",
            "|bank_demo_db|bank_marketing_predictions|false      |\n",
            "+------------+--------------------------+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}